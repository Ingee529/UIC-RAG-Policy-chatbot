# -*- coding: utf-8 -*-
"""MGR (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TDSUivSSHYts6E0R4U0CVkuGoH33IzJ1
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Specify the path to your folder in Google Drive containing the policy files
# Replace 'Your_Policy_Files_Folder' with the actual path to your folder
drive_folder_path = '/content/drive/MyDrive/Budget_Additional_Resources-selected'

# List files in the specified folder to verify
try:
    uploaded_files = [f for f in os.listdir(drive_folder_path) if os.path.isfile(os.path.join(drive_folder_path, f))]
    print("Files found in your Google Drive folder:", uploaded_files)
except FileNotFoundError:
    print(f"Error: The folder '{drive_folder_path}' was not found in your Google Drive.")
    print("Please create this folder and upload your policy files there.")
    uploaded_files = []

# Now 'uploaded_files' contains the list of filenames you can use in the subsequent cells

!pip install tldextract unidecode requests beautifulsoup4 pandas urllib3 collections

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import json
import time
import pandas as pd
from collections import deque
import os

def get_page_content(url, session):
    """Get page content with error handling"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = session.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"  Error fetching {url}: {e}")
        return None

def extract_all_links(html, base_url):
    """Extract all relevant links from the page"""
    soup = BeautifulSoup(html, 'html.parser')
    links = []

    # Look for links in main content areas
    content_selectors = [
        'main a',
        '.content a',
        '.main-content a',
        'article a',
        '.entry-content a',
        'div[role="main"] a',
        'a.bfpp-link',  # Specific to BFPP site
    ]

    for selector in content_selectors:
        for link in soup.select(selector):
            href = link.get('href')
            if href:
                absolute_url = urljoin(base_url, href)
                link_text = link.get_text(strip=True)
                if link_text and len(link_text) > 3:  # Filter out very short links
                    links.append({
                        'text': link_text,
                        'url': absolute_url,
                        'full_text': link.get_text()
                    })

    # Also get navigation links if main content is empty
    if not links:
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and not href.startswith(('javascript:', 'mailto:', '#')):
                absolute_url = urljoin(base_url, href)
                link_text = link.get_text(strip=True)
                if link_text and len(link_text) > 3:
                    links.append({
                        'text': link_text,
                        'url': absolute_url,
                        'full_text': link.get_text()
                    })

    return links

def extract_main_content(html, url):
    """Extract main content from the page"""
    soup = BeautifulSoup(html, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Try to find main content areas
    content_selectors = [
        'main',
        '.content',
        '.main-content',
        'article',
        '.entry-content',
        'div[role="main"]',
        '#main-content',
        '.bfpp-content',  # Specific to BFPP site
    ]

    main_content = None
    for selector in content_selectors:
        main_content = soup.select_one(selector)
        if main_content:
            break

    # If no specific content area found, use body but remove navigation
    if not main_content:
        main_content = soup.find('body')
        # Remove common navigation elements
        for nav in main_content.select('nav, header, footer, .navbar, .sidebar'):
            nav.decompose()

    if main_content:
        text = main_content.get_text(separator='\n', strip=True)
        # Clean up excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        return text.strip()

    return ""

def is_subsection_link(link_text, current_section):
    """Check if a link is likely a subsection"""
    # Look for numbered subsections (like 1.1, 1.2, etc.)
    section_pattern = r'^\d+\.\d+'
    if re.match(section_pattern, link_text):
        return True

    # Look for links that contain the section number
    current_section_num = re.search(r'^(\d+)', current_section)
    if current_section_num:
        section_num = current_section_num.group(1)
        if section_num in link_text and len(link_text) > 5:
            return True

    # Look for common subsection patterns
    subsection_keywords = ['policy', 'procedure', 'guideline', 'compliance', 'reporting', 'conducting']
    if any(keyword in link_text.lower() for keyword in subsection_keywords):
        return True

    return False

def scrape_section_with_subsections(root_url, section_title, session):
    """Scrape a section and all its subsections"""
    print(f"=== Processing {section_title} ===")
    print(f"Root URL: {root_url}")

    all_pages = []

    # Get root page
    html = get_page_content(root_url, session)
    if not html:
        print(f"  Failed to fetch root page")
        return all_pages

    # Extract root page content
    root_content = extract_main_content(html, root_url)
    print(f"  Root page: {len(root_content)} chars extracted")

    if len(root_content) < 100:
        print("  âš  Short content in root page, will look for subsections")

    # Save root page
    all_pages.append({
        'section': section_title,
        'subsection': 'Overview',
        'url': root_url,
        'title': section_title,
        'content': root_content,
        'content_length': len(root_content)
    })

    # Extract all links
    links = extract_all_links(html, root_url)
    print(f"  Found {len(links)} total links")

    # Filter for subsection links
    subsection_links = []
    other_links = []

    for link in links:
        if is_subsection_link(link['text'], section_title):
            subsection_links.append(link)
        else:
            other_links.append(link)

    print(f"  Identified {len(subsection_links)} potential subsection links")
    print(f"  Other links: {len(other_links)}")

    # Scrape subsection pages
    for i, link in enumerate(subsection_links):
        print(f"    Subsection {i+1}: {link['text']}")
        print(f"      URL: {link['url']}")

        # Avoid duplicate scraping
        if any(page['url'] == link['url'] for page in all_pages):
            print("      âš  Already scraped, skipping")
            continue

        time.sleep(1)  # Be polite

        sub_html = get_page_content(link['url'], session)
        if sub_html:
            sub_content = extract_main_content(sub_html, link['url'])
            print(f"      Extracted: {len(sub_content)} chars")

            all_pages.append({
                'section': section_title,
                'subsection': link['text'],
                'url': link['url'],
                'title': f"{section_title} - {link['text']}",
                'content': sub_content,
                'content_length': len(sub_content)
            })
        else:
            print("      Failed to fetch subsection")

    # Also scrape some other relevant links (like Ethics and Compliance)
    relevant_other_links = [link for link in other_links if len(link['text']) > 10]
    for i, link in enumerate(relevant_other_links[:5]):  # Limit to first 5 to avoid too many requests
        print(f"    Additional link {i+1}: {link['text']}")

        if any(page['url'] == link['url'] for page in all_pages):
            continue

        time.sleep(1)

        sub_html = get_page_content(link['url'], session)
        if sub_html:
            sub_content = extract_main_content(sub_html, link['url'])
            print(f"      Extracted: {len(sub_content)} chars")

            all_pages.append({
                'section': section_title,
                'subsection': link['text'],
                'url': link['url'],
                'title': f"{section_title} - {link['text']}",
                'content': sub_content,
                'content_length': len(sub_content)
            })

    print(f"  Total pages for {section_title}: {len(all_pages)}")
    return all_pages

# Main scraping function
def scrape_bfpp_comprehensive():
    """Comprehensive scraper that gets all sections and subsections"""

    sections = {
        "1 Fiscal Environment": "https://www.busfin.uillinois.edu/bfpp/section-1-intro-business-financial-functions",
        "2 Custodial Funds": "https://www.busfin.uillinois.edu/bfpp/section-2-agency-funds",
        "3 Budget": "https://www.busfin.uillinois.edu/bfpp/section-3-budget",
        "4 Payroll": "https://www.busfin.uillinois.edu/bfpp/section-4-payroll",
        "5 Receivables": "https://www.busfin.uillinois.edu/bfpp/section-5-receivables",
        "6 Insurance": "https://www.busfin.uillinois.edu/bfpp/section-6-insurance",
        "7 Purchasing": "https://www.busfin.uillinois.edu/bfpp/section-7-purchasing",
        "8 Payments and Reimbursements": "https://www.busfin.uillinois.edu/bfpp/section-8-payments-reimbursements",
        "9 Audits, Internal Control, and Business System Security": "https://www.busfin.uillinois.edu/bfpp/section-9-audits-internal-control",
        "10 Cash Handling": "https://www.busfin.uillinois.edu/bfpp/section-10-cash-handling",
        "11 Gifts and Endowments": "https://www.busfin.uillinois.edu/bfpp/section-11-gifts-endowments",
        "12 Property Accounting": "https://www.busfin.uillinois.edu/bfpp/section-12-property-accounting",
        "13 Accounting": "https://www.busfin.uillinois.edu/bfpp/section-13-accounting",
        "14 Investments, Banking, and Internal Loans": "https://www.busfin.uillinois.edu/bfpp/section-14-investments",
        "15 Travel": "https://www.busfin.uillinois.edu/bfpp/section-15-travel",
        "16 Sponsored Programs": "https://www.busfin.uillinois.edu/bfpp/section-16-grants-research-contracts",
        "17 Consultants and Independent Contractors": "https://www.busfin.uillinois.edu/bfpp/section-17-consultants-contractors",
        "18 Taxes": "https://www.busfin.uillinois.edu/bfpp/section-18-taxes",
        "19 Contracts": "https://www.busfin.uillinois.edu/bfpp/section-19-contracts",
        "20 ID Cards, UINs, and Data Services": "https://www.busfin.uillinois.edu/bfpp/section-20-id-cards-uins-data-services",
        "21 Merchant Cards": "https://www.busfin.uillinois.edu/bfpp/section-21-merchant-cards",
        "22 Self-Supporting Funds": "https://www.busfin.uillinois.edu/bfpp/22_self-supporting_funds"
    }

    session = requests.Session()
    all_pages_data = []

    for section_title, section_url in sections.items():
        section_pages = scrape_section_with_subsections(section_url, section_title, session)
        all_pages_data.extend(section_pages)
        time.sleep(2)  # Be polite between sections

    return all_pages_data

# Run the comprehensive scraper
print("Starting comprehensive BFPP scraping...")
all_pages = scrape_bfpp_comprehensive()

# Create DataFrame
df = pd.DataFrame(all_pages)

# Display summary
print(f"\n=== Summary ===")
print(f"Total pages scraped: {len(df)}")
print(f"Total content characters: {df['content_length'].sum()}")
print(f"Average content per page: {df['content_length'].mean():.0f} chars")

# Show sections with most content
section_stats = df.groupby('section').agg({
    'subsection': 'count',
    'content_length': 'sum'
}).sort_values('content_length', ascending=False)

print(f"\nTop sections by content:")
print(section_stats.head(10))

# Save results
output_dir = "/content/drive/MyDrive/BFPP_Scrape_Comprehensive"
os.makedirs(output_dir, exist_ok=True)

# Save CSV
csv_path = os.path.join(output_dir, "bfpp_comprehensive_pages.csv")
df.to_csv(csv_path, index=False)
print(f"\nâœ“ Saved {len(df)} pages to {csv_path}")

# Save detailed JSON
json_path = os.path.join(output_dir, "bfpp_comprehensive_data.json")
with open(json_path, 'w', encoding='utf-8') as f:
    json.dump(all_pages, f, indent=2, ensure_ascii=False)
print(f"âœ“ Saved detailed data to {json_path}")

print("\n[DONE] Comprehensive scraping completed!")

# Install required libraries (skip in GCP container if pre-installed)
!pip install -q python-docx PyMuPDF sentence-transformers faiss-cpu \
                transformers mistralai openai beautifulsoup4 lxml tldextract \
                rapidfuzz wordcloud plotly

import os
import fitz  # PyMuPDF
import docx
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import random
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from mistralai.client import MistralClient
import openai
from bs4 import BeautifulSoup
import requests
import tldextract
from urllib.parse import urljoin, urlparse
from wordcloud import WordCloud

# === PART 1: PARSING ===
# âœ… Unified code for local files + scraped pages

import os
import fitz  # PyMuPDF
import docx
import json
from bs4 import BeautifulSoup

# --- Document Parser ---
def parse_document(filepath, source_url=None, is_web=False, html_text=None):
    blocks = []
    current_block = {"heading": None, "text": ""}

    if is_web and html_text:
        soup = BeautifulSoup(html_text, "html.parser")
        paragraphs = soup.find_all(['h1','h2','h3','h4','h5','p','li'])
        for tag in paragraphs:
            text = tag.get_text(strip=True)
            if not text: continue
            if tag.name.startswith('h'):
                if current_block["heading"] or current_block["text"]:
                    blocks.append(current_block)
                current_block = {"heading": text, "text": ""}
            else:
                current_block["text"] += ("\n" if current_block["text"] else "") + text
        if current_block["heading"] or current_block["text"]:
            blocks.append(current_block)

    elif filepath.lower().endswith(".docx"):
        doc = docx.Document(filepath)
        for para in doc.paragraphs:
            text = para.text.strip()
            if not text: continue
            style = para.style.name if para.style else ""
            if style.startswith("Heading"):
                if current_block["heading"] or current_block["text"]:
                    blocks.append(current_block)
                current_block = {"heading": text, "text": ""}
            else:
                current_block["text"] += ("\n" if current_block["text"] else "") + text
        if current_block["heading"] or current_block["text"]:
            blocks.append(current_block)

    elif filepath.lower().endswith(".pdf"):
        doc = fitz.open(filepath)
        full_text = "".join(page.get_text() for page in doc)
        lines = full_text.splitlines()
        para_buffer, paragraphs = [], []
        for line in lines:
            if not line.strip():
                if para_buffer:
                    paragraphs.append(" ".join(para_buffer).strip())
                    para_buffer = []
            else:
                para_buffer.append(line.strip())
        if para_buffer:
            paragraphs.append(" ".join(para_buffer).strip())

        for para in paragraphs:
            is_heading = (len(para) < 40 or para.endswith(":") or para.isupper())
            if is_heading:
                if current_block["heading"] or current_block["text"]:
                    blocks.append(current_block)
                current_block = {"heading": para.strip(":"), "text": ""}
            else:
                current_block["text"] += ("\n" if current_block["text"] else "") + para
        if current_block["heading"] or current_block["text"]:
            blocks.append(current_block)

    else:
        raise ValueError(f"Unsupported or missing format: {filepath}")

    for blk in blocks:
        blk["source_url"] = source_url
    return blocks

# --- Scraped JSON Parser ---
def chunk_text(text, max_chars=800):
    import re
    chunks, buffer = [], ""
    for para in re.split(r"\n+", text):
        para = para.strip()
        if not para:
            continue
        if len(buffer) + len(para) + 1 > max_chars:
            if buffer:
                chunks.append(buffer.strip())
                buffer = ""
        buffer += para + "\n"
    if buffer:
        chunks.append(buffer.strip())
    return chunks

def parse_scraped_pages(scraped_pages):
    parsed_blocks = []
    for page in scraped_pages:
        content = page.get("content", "").strip()
        if not content or len(content) < 100:
            continue
        section = page.get("section", "Unknown Section")
        subsection = page.get("subsection", "Unknown Subsection")
        url = page.get("url", None)
        for blk in chunk_text(content):
            parsed_blocks.append({
                "section": section,
                "subsection": subsection,
                "text": blk,
                "source_url": url
            })
    return parsed_blocks

# --- Run Parser ---
drive_folder_path = '/content/drive/MyDrive/Budget_Additional_Resources-selected'
uploaded_files = os.listdir(drive_folder_path)
scraped_json_path = '/content/drive/MyDrive/BFPP_Scrape_Comprehensive/bfpp_comprehensive_data.json'

with open(scraped_json_path, 'r') as f:
    scraped_pages = json.load(f)

documents_blocks = {}

# Local file parsing
for filename in uploaded_files:
    filepath = os.path.join(drive_folder_path, filename)
    try:
        blocks = parse_document(filepath)
        documents_blocks[filename] = blocks
        print(f"âœ… Parsed file: {filename} â†’ {len(blocks)} blocks")
    except Exception as e:
        print(f"âŒ Error parsing {filename}: {e}")

# Web page parsing
# Inject parsed text chunks into documents_blocks using parse_scraped_pages
scraped_blocks = parse_scraped_pages(scraped_pages)
print(f"ðŸŒ Parsed {len(scraped_blocks)} blocks from {len(scraped_pages)} scraped pages")

# Group blocks by page to mimic file-style structure
from collections import defaultdict
grouped_by_page = defaultdict(list)
for i, blk in enumerate(scraped_blocks):
    url = blk.get("source_url", f"web_{i}")
    grouped_by_page[url].append({
        "heading": f"{blk.get('section', '')} - {blk.get('subsection', '')}",
        "text": blk.get("text", ""),
        "source_url": url
    })

# Add to documents_blocks with unique keys
for idx, (url, blocks) in enumerate(grouped_by_page.items()):
    doc_key = f"web_{idx+1}"
    documents_blocks[doc_key] = blocks
    print(f"âœ… Added scraped page: {doc_key} â†’ {len(blocks)} blocks")

print(f"ðŸŒ Parsed {len(scraped_blocks)} blocks from {len(scraped_pages)} scraped pages")

"""**Chunking, Metadata Enhancement, and FAISS Ingestion**"""

!pip install -q sentence-transformers tiktoken

from sentence_transformers import SentenceTransformer
import numpy as np
import tiktoken

# Initialize the sentence embedding model (Sentence-BERT) and tokenizer for token counting
model = SentenceTransformer('all-MiniLM-L6-v2')
token_encoder = tiktoken.get_encoding("cl100k_base")  # GPT-3.5/GPT-4 encoding

# Example input: documents_blocks is a dictionary where keys are filenames or web labels (URLs),
# and values are lists of parsed blocks (each block may contain text and optionally headings).
# documents_blocks = {
#     "example.pdf": [ {"text": "First paragraph...", "headings": ["Introduction"]}, ... ],
#     "https://example.com/page": [ {"text": "Paragraph 1...", "headings": ["Section 1", "Subsection A"]}, ... ]
# }

final_chunks = []  # List to collect the resulting chunks across all documents

total_chunks = 0
for source_key, blocks in documents_blocks.items():
    if not blocks:  # Skip if no content blocks
        continue

    # Determine source label and source URL for this document
    source_label = source_key
    source_url = None
    if source_key.startswith("http://") or source_key.startswith("https://"):
        source_url = source_key
        # Use the URL itself as the label, or customize if needed (e.g., domain or title)
        # Here we keep the full URL as source_label for web content
        source_label = source_key

    # Extract text from all blocks for embedding, and compute token lengths for each block
    texts = [block.get("text", "") for block in blocks]
    block_token_counts = [len(token_encoder.encode(text)) for text in texts]

    # Compute embeddings for all blocks using Sentence-BERT
    embeddings = model.encode(texts, convert_to_numpy=True)
    # Pre-normalize embeddings for cosine similarity calculation
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    # Avoid division by zero in case of empty text blocks
    norms[norms == 0] = 1e-9
    normalized_embeddings = embeddings / norms

    # Merge adjacent blocks based on cosine similarity threshold and token limit
    doc_chunk_count = 0
    i = 0
    while i < len(blocks):
        # Start a new chunk with block i
        current_text = blocks[i].get("text", "")
        current_headings = []
        # If the block has headings, collect them (as list or string)
        if "headings" in blocks[i] and blocks[i]["headings"]:
            # Ensure we have a list of headings
            if isinstance(blocks[i]["headings"], list):
                current_headings = list(blocks[i]["headings"])
            else:
                current_headings = [blocks[i]["headings"]]

        current_tokens = block_token_counts[i]
        # Merge subsequent blocks while similarity is high and token limit not exceeded
        j = i + 1
        while j < len(blocks):
            # Compute cosine similarity between current block j and previous block (j-1) in this chunk
            cos_sim = np.dot(normalized_embeddings[j], normalized_embeddings[j-1])
            # If similarity is above threshold and merging won't exceed token limit, merge the block
            if cos_sim >= 0.75 and current_tokens + block_token_counts[j] <= 400:
                # Append the block's text to current chunk text (separate by newline for clarity)
                current_text += "\n" + blocks[j].get("text", "")
                current_tokens += block_token_counts[j]
                # Collect headings from block j if present
                if "headings" in blocks[j] and blocks[j]["headings"]:
                    if isinstance(blocks[j]["headings"], list):
                        for h in blocks[j]["headings"]:
                            # Append heading if not already in current_headings
                            if h not in current_headings:
                                current_headings.append(h)
                    else:
                        h = blocks[j]["headings"]
                        if h not in current_headings:
                            current_headings.append(h)
                # Move to next block
                j += 1
            else:
                # Stop merging if similarity drops below threshold or token limit would be exceeded
                break
        # Finalize the chunk for blocks i through j-1
        chunk_tokens = len(token_encoder.encode(current_text))
        headings_str = " > ".join(current_headings) if current_headings else ""
        chunk = {
            "source": source_label,
            "headings": headings_str,
            "text": current_text,
            "tokens": chunk_tokens
        }
        if source_url:
            chunk["source_url"] = source_url

        final_chunks.append(chunk)
        doc_chunk_count += 1

        # Continue from the next new block after this chunk
        i = j

    # Print chunk count for this document
    print(f"{source_label}: {doc_chunk_count} chunks")
    total_chunks += doc_chunk_count

print(f"Total chunks: {total_chunks}")

import re
from sentence_transformers import util

def recursive_chunk_text(text, max_size=400, overlap=100):
    """Split text into overlapping sentence-based chunks."""
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())
    chunks, current_chunk = [], []
    token_count = lambda txt: len(txt.split())

    for sent in sentences:
        current_chunk.append(sent)
        if token_count(" ".join(current_chunk)) >= max_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = current_chunk[-overlap:]
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks


def semantic_chunk_text(text, max_size=400, model=None):
    """Split text where semantic drift is detected."""
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())
    if len(sentences) <= 1:
        return [text]

    embeddings = model.encode(sentences)
    chunks, buffer, current_tokens = [], [], 0

    for i, sent in enumerate(sentences):
        buffer.append(sent)
        current_tokens += len(sent.split())

        drift = (i < len(sentences)-1 and util.cos_sim(embeddings[i], embeddings[i+1]) < 0.2)
        if current_tokens >= max_size or drift:
            chunks.append(" ".join(buffer))
            buffer, current_tokens = [], 0

    if buffer:
        chunks.append(" ".join(buffer))
    return chunks


def sliding_token_chunks(text, max_tokens=350, stride=100):
    """Strict token-sized chunks with stride."""
    words = text.strip().split()
    chunks = []
    for i in range(0, len(words), max_tokens - stride):
        chunk = " ".join(words[i:i + max_tokens])
        if chunk:
            chunks.append(chunk)
    return chunks

def chunk_block(block, doc_name, model=None, strategy="auto"):
    """
    Decide chunking strategy based on block characteristics.
    Returns list of chunk dicts.
    """
    text = block.get("text", "").strip()
    heading = block.get("heading")
    source_url = block.get("source_url")
    pdf_path = block.get("pdf_path")

    if not text:
        return []

    # Strategy decision
    if strategy == "heading" and heading:
        chunks = recursive_chunk_text(text)
    elif strategy == "semantic":
        chunks = semantic_chunk_text(text, model=model)
    elif strategy == "sliding":
        chunks = sliding_token_chunks(text)
    elif strategy == "recursive":
        chunks = recursive_chunk_text(text)
    elif strategy == "auto":
        if heading:  # structured doc
            chunks = recursive_chunk_text(text)
        elif len(text.split()) > 600:
            chunks = semantic_chunk_text(text, model=model)
        else:
            chunks = recursive_chunk_text(text)
    else:
        raise ValueError(f"Unknown chunking strategy: {strategy}")

    chunk_dicts = []
    for ch in chunks:
        chunk_dicts.append({
            "doc": doc_name,
            "heading": heading,
            "content": ch,
            "source_url": source_url,
            "pdf_path": pdf_path
        })
    return chunk_dicts

# Load embedding model once for semantic chunking
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# Build all_chunks
all_chunks = []

for doc_name, blocks in documents_blocks.items():
    for block in blocks:
        # Strategy: auto-pick based on structure and size
        chunks = chunk_block(block, doc_name, model=embed_model, strategy="auto")
        all_chunks.extend(chunks)

print(f"âœ… Total chunks generated: {len(all_chunks)}")

# === PART 3: Embedding & FAISS Indexing ===

!pip install -q faiss-cpu sentence-transformers

import os
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

# --- Setup ---
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
output_dir = "/content/drive/MyDrive/META_Index"
os.makedirs(output_dir, exist_ok=True)

# --- Validate Chunk Data ---
try:
    final_chunks
except NameError:
    raise ValueError("âš ï¸ `final_chunks` not found. Please run Part 2 (chunking) before this.")

# --- Prepare Embeddings and Metadata ---
texts = [chunk["text"] for chunk in final_chunks]
metadata = [{
    "source": chunk.get("source"),
    "source_url": chunk.get("source_url"),
    "headings": chunk.get("headings"),
    "tokens": chunk.get("tokens")
} for chunk in final_chunks]

print(f"ðŸ” Embedding {len(texts)} chunks...")

embeddings = embedding_model.encode(texts, show_progress_bar=True)
embeddings = np.array(embeddings).astype("float32")

# --- Create FAISS Index ---
embedding_dim = embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(embeddings)

# --- Save Index and Metadata ---
faiss.write_index(index, os.path.join(output_dir, "faiss_index.idx"))

with open(os.path.join(output_dir, "meta.pkl"), "wb") as f:
    pickle.dump({
        "texts": texts,
        "metadata": metadata
    }, f)

print("âœ… FAISS index created and saved.")
print(f"ðŸ“¦ {len(texts)} chunks embedded.")
print(f"ðŸ§  Index dimension: {embedding_dim}")

!pip install --upgrade openai

import openai
import json
import time

# Set API key from environment variables
from openai import OpenAI
import os
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))  # Securely load from environment

def extract_metadata_openai_v1(text: str) -> dict:
    """Use OpenAI (v1 API) to extract structured metadata from text."""

    system_prompt = (
        "You are a helpful assistant that extracts structured metadata from a policy chunk. "
        "Always return a valid JSON with only these keys:\n"
        "summary, keywords, entities, effective_date, fund_codes, ilcs_citations, "
        "title, category, sub_category, topic, year, content_type."
    )

    user_prompt = (
        f"Text:\n\"\"\"\n{text}\n\"\"\"\n\n"
        "Extract the metadata fields into JSON as described above. "
        "Use empty string or list for missing fields. No explanations, only JSON."
    )

    try:
        chat_response = client.chat.completions.create(
            model="gpt-3.5-turbo",  # or gpt-4
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0,
            max_tokens=500
        )

        output = chat_response.choices[0].message.content.strip()
        if output.startswith("```json"):
            output = output[7:]
        if output.endswith("```"):
            output = output[:-3]
        metadata = json.loads(output)

    except Exception as e:
        print(f"[extract_metadata_openai_v1] Error: {e}")
        metadata = {}

    # Ensure all keys
    expected_keys = [
        "summary", "keywords", "entities", "effective_date", "fund_codes",
        "ilcs_citations", "title", "category", "sub_category", "topic",
        "year", "content_type"
    ]
    for k in expected_keys:
        if k not in metadata:
            metadata[k] = [] if k in ["keywords", "entities", "fund_codes", "ilcs_citations"] else ""
    return metadata

import openai
import json
import time
import os

# Set your API key from environment variables
openai.api_key = os.getenv("OPENAI_API_KEY")

def extract_metadata_openai(text: str) -> dict:
    """
    Use OpenAI (GPT-3.5 or GPT-4) to extract structured metadata from policy text.
    Always returns a dictionary with required metadata fields.
    """
    system_prompt = (
        "You are a helpful assistant that extracts specified metadata from policy text. "
        "Only return JSON with keys listed."
    )
    user_prompt = (
        f"Text:\n\"\"\"\n{text}\n\"\"\"\n"
        "Extract the following information and output as JSON with these keys only:\n"
        "summary, keywords, entities, effective_date, fund_codes, ilcs_citations, "
        "title, category, sub_category, topic, year, content_type.\n\n"
        "- summary: a brief summary of the text\n"
        "- keywords: a list of important keywords or phrases\n"
        "- entities: named people, departments, organizations\n"
        "- effective_date: date the policy takes effect\n"
        "- fund_codes: any account/fund numbers\n"
        "- ilcs_citations: ILCS references like '5 ILCS 430/...'\n"
        "- title: document or policy title\n"
        "- category: broad type of policy (e.g., Financial, Academic)\n"
        "- sub_category: specific area if applicable\n"
        "- topic: main subject of the policy\n"
        "- year: year policy refers to\n"
        "- content_type: type of document (policy, guideline, etc.)\n"
        "If a field is not present, return empty string or list.\n"
        "Only return valid JSON. No explanations."
    )

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # or "gpt-4"
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0,
            max_tokens=500
        )

        output_text = response.choices[0].message.content.strip()
        if output_text.startswith("```json"):
            output_text = output_text[7:]
        if output_text.endswith("```"):
            output_text = output_text[:-3]
        metadata = json.loads(output_text)

    except Exception as e:
        print(f"[extract_metadata_openai] Error: {e}")
        metadata = {}

    # Ensure all required keys are present
    expected_keys = [
        "summary", "keywords", "entities", "effective_date", "fund_codes",
        "ilcs_citations", "title", "category", "sub_category", "topic",
        "year", "content_type"
    ]
    for k in expected_keys:
        if k not in metadata:
            metadata[k] = [] if k in ["keywords", "entities", "fund_codes", "ilcs_citations"] else ""
    return metadata

# Enrich chunks
enriched_chunks = []
for i, chunk in enumerate(final_chunks):
    print(f"ðŸ” Extracting metadata for chunk {i + 1}/{len(final_chunks)}...")
    meta = extract_metadata_openai_v1(chunk.get("text", ""))
    chunk.update(meta)
    enriched_chunks.append(chunk)
    time.sleep(0.9)

# Part 4: RAG pipeline - Query embedding, retrieval, and chat completion

import os
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import pickle

# Initialize OpenAI client and model
openai_api_key = os.getenv("OPENAI_API_KEY", "YOUR_API_KEY")
client = OpenAI(api_key=openai_api_key)
use_gpt4 = False  # Set True to use GPT-4 if available
chat_model = "gpt-4" if use_gpt4 else "gpt-3.5-turbo"

# Initialize embedding model (SentenceTransformers) if needed
use_openai_embeddings = False  # Set True to use OpenAI embeddings instead of SentenceTransformers
openai_embed_model = "text-embedding-ada-002"
st_model = None
if not use_openai_embeddings:
    st_model = SentenceTransformer('all-MiniLM-L6-v2')

# Load FAISS index
faiss_index_path = "/content/drive/MyDrive/META_Index/faiss_index.idx"  # replace with your FAISS index file path
try:
    index = faiss.read_index(faiss_index_path)
except Exception as e:
    print(f"Error loading FAISS index from '{faiss_index_path}': {e}")
    index = None

# Load chunk metadata (list of dicts with 'source_url', 'heading', 'content'/'text')
chunks_path = "/content/drive/MyDrive/META_Index/meta.pkl"  # replace with your chunk metadata file path
try:
    with open(chunks_path, "rb") as f:
        chunks = pickle.load(f)
except Exception as e:
    print(f"Error loading chunk data from '{chunks_path}': {e}")
    chunks = []

def embed_query(query: str) -> np.ndarray:
    """
    Embed the query using either OpenAI embeddings or SentenceTransformers.
    """
    if use_openai_embeddings:
        try:
            response = client.embeddings.create(model=openai_embed_model, input=query)
            data = response.get("data", None)
            if data:
                embedding = np.array(data[0]["embedding"])
            else:
                embedding = np.array(response.data[0].embedding)
        except Exception as e:
            print(f"OpenAI embedding error: {e}")
            return None
    else:
        if st_model is None:
            print("SentenceTransformer model not initialized.")
            return None
        embedding = st_model.encode([query])[0]
    # Normalize embedding
    norm = np.linalg.norm(embedding)
    if norm > 0:
        embedding = embedding / norm
    return embedding

def retrieve_relevant_chunks(query_embedding: np.ndarray, top_k: int = 5):
    """
    Retrieve top_k relevant chunks from the FAISS index given the query embedding.
    """
    if index is None or not chunks:
        print("FAISS index or chunks data not available.")
        return []
    query_vec = np.array([query_embedding]).astype("float32")
    try:
        distances, indices = index.search(query_vec, top_k)
    except Exception as e:
        print(f"FAISS search error: {e}")
        return []
    indices = indices.flatten().tolist()
    indices = [idx for idx in indices if idx != -1 and idx < len(chunks)]
    relevant_chunks = []
    for idx in indices:
        chunk = chunks[idx]
        content = chunk.get("content", chunk.get("text", ""))
        relevant_chunks.append({
            "source_url": chunk.get("source_url", ""),
            "heading": chunk.get("heading", ""),
            "content": content
        })
    return relevant_chunks

def construct_prompt(query: str, context_chunks: list) -> tuple:
    """
    Construct the system and user messages for the chat model, including citation instructions.
    """
    system_prompt = (
        "You are an AI assistant that answers questions based on provided document context. "
        "Use the context to support your answer, and include inline citations for any referenced information. "
        "Citations should reference the source URL and heading of the context chunk in brackets, like [source_url - heading]. "
        "If information is not in the context, say you don't know."
    )
    context_text = ""
    for i, chunk in enumerate(context_chunks, 1):
        src = chunk["source_url"] or "Unknown Source"
        head = chunk["heading"] or "No Heading"
        content = chunk["content"]
        context_text += f"[Document {i}] {content} (Source: {src}, Heading: {head})\n\n"
    user_prompt = f"Question: {query}\n\nContext:\n{context_text}"
    return system_prompt, user_prompt

# Meta-RAG enhancement stub
def meta_rag_enhance(query: str, docs_metadata: list):
    """
    Placeholder for Meta-RAG logic (e.g., function calling or metadata-based rerouting).
    """
    # Example: route query to a special function based on metadata
    return None

# GEAR graph-based reasoning stub
def graph_reasoning_stub(query: str):
    """
    Placeholder for graph-based (GEAR) multi-hop reasoning (e.g., expand query via entity graph).
    """
    # Example: multi-hop query expansion using a knowledge graph
    return None

# Main execution
try:
    user_query = "YOUR QUERY HERE"  # replace with your actual query
    query_emb = embed_query(user_query)
    if query_emb is None:
        raise ValueError("Query embedding failed.")
    top_chunks = retrieve_relevant_chunks(query_emb, top_k=5)
    if not top_chunks:
        print("No relevant chunks found.")
    system_msg, user_msg = construct_prompt(user_query, top_chunks)
    response = client.chat.completions.create(
        model=chat_model,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ]
    )
    answer = response.choices[0].message.content.strip()
    print("Answer:\n", answer)
except Exception as e:
    print(f"RAG pipeline error: {e}")