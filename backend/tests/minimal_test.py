#!/usr/bin/env python3
"""
æ¥µç°¡RAGæ¸¬è©¦ - å®Œå…¨é¿é–‹å¤šåŸ·è¡Œç·’å•é¡Œ
ä½¿ç”¨æœ€å°ä¾è³´ï¼Œå…ˆè¨­ç½®æ‰€æœ‰ç’°å¢ƒè®Šæ•¸å†è¼‰å…¥ä»»ä½•åº«
"""

# ç¬¬ä¸€æ­¥ï¼šåœ¨è¼‰å…¥ä»»ä½•åº«ä¹‹å‰è¨­ç½®æ‰€æœ‰ç’°å¢ƒè®Šæ•¸
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TORCH_NUM_THREADS"] = "1"

# ç¬¬äºŒæ­¥ï¼šç¾åœ¨æ‰è¼‰å…¥å…¶ä»–åº«
import sys
import json
import pickle
import numpy as np
from pathlib import Path

print("=" * 80)
print("ğŸš€ UIC Policy Assistant - æ¥µç°¡æ¸¬è©¦")
print("=" * 80)

# ç¬¬ä¸‰æ­¥ï¼šæœ€å¾Œæ‰è¼‰å…¥å¯èƒ½æœ‰å•é¡Œçš„åº«
print("\nğŸ“¦ è¼‰å…¥æ¨¡å‹åº«...")

# å…ˆè¨­ç½®FAISSä¸ä½¿ç”¨å¤šåŸ·è¡Œç·’
import faiss
faiss.omp_set_num_threads(1)
print("âœ… FAISSå·²è¨­ç½®ç‚ºå–®åŸ·è¡Œç·’")

# æœ€å¾Œæ‰è¼‰å…¥SentenceTransformerï¼Œä¸”è¨­ç½®ç‚ºå–®åŸ·è¡Œç·’
print("ğŸ“¦ è¼‰å…¥SentenceTransformer (å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“)...")
import torch
torch.set_num_threads(1)

from sentence_transformers import SentenceTransformer
print("âœ… SentenceTransformerå·²è¼‰å…¥")

def main():
    BACKEND_DIR = Path(__file__).parent

    # è¼‰å…¥æ¸¬è©¦å•é¡Œ
    queries_file = BACKEND_DIR / "test_queries.json"
    print(f"\nğŸ“‚ è¼‰å…¥æ¸¬è©¦å•é¡Œ: {queries_file}")
    with open(queries_file, 'r') as f:
        queries = json.load(f)
    print(f"âœ… è¼‰å…¥äº† {len(queries)} å€‹æ¸¬è©¦å•é¡Œ\n")

    for qid, query_text in queries.items():
        print(f"{qid}: {query_text}")

    # è¨­ç½®è·¯å¾‘
    embedding_dir = BACKEND_DIR / "embeddings_output" / "naive" / "naive_embedding"

    # è¼‰å…¥FAISSç´¢å¼•
    print(f"\nğŸ“‚ è¼‰å…¥FAISSç´¢å¼•...")
    index_path = embedding_dir / "index.faiss"
    index = faiss.read_index(str(index_path))
    print(f"âœ… FAISSç´¢å¼•: {index.ntotal} å€‹å‘é‡, ç¶­åº¦: {index.d}")

    # è¼‰å…¥IDæ˜ å°„
    print(f"\nğŸ“‚ è¼‰å…¥IDæ˜ å°„...")
    id_mapping_path = embedding_dir / "id_mapping.pkl"
    with open(id_mapping_path, 'rb') as f:
        mapping_data = pickle.load(f)
        index_to_id = mapping_data['index_to_id']
    print(f"âœ… IDæ˜ å°„: {len(index_to_id)} å€‹")

    # è¼‰å…¥å…ƒæ•¸æ“š
    print(f"\nğŸ“‚ è¼‰å…¥å…ƒæ•¸æ“š...")
    metadata_path = embedding_dir / "metadata.json"
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    print(f"âœ… å…ƒæ•¸æ“š: {len(metadata)} å€‹chunks")

    # è¼‰å…¥embeddingæ¨¡å‹ - é€™æ˜¯æœ€å¯èƒ½å¡ä½çš„åœ°æ–¹
    print(f"\nğŸ“‚ è¼‰å…¥embeddingæ¨¡å‹ (é€™å¯èƒ½éœ€è¦30-60ç§’ï¼Œè«‹è€å¿ƒç­‰å¾…)...")
    model_name = metadata.get('model_name', 'Snowflake/arctic-embed-s')
    print(f"   æ¨¡å‹: {model_name}")

    try:
        model = SentenceTransformer(model_name, device='cpu')  # å¼·åˆ¶ä½¿ç”¨CPU
        print(f"âœ… Embeddingæ¨¡å‹å·²è¼‰å…¥")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return

    # é–‹å§‹æ¸¬è©¦
    print("\n" + "=" * 80)
    print("ğŸ“ é–‹å§‹æ¸¬è©¦ 5 å€‹å•é¡Œ")
    print("=" * 80)

    all_results = {}

    for qid, query_text in queries.items():
        print(f"\n{'='*80}")
        print(f"ğŸ” {qid}: {query_text}")
        print("="*80)

        try:
            # Encode query
            print("   æ­£åœ¨ç·¨ç¢¼æŸ¥è©¢...")
            query_embedding = model.encode([query_text], convert_to_numpy=True, show_progress_bar=False)
            print(f"   âœ… æŸ¥è©¢å·²ç·¨ç¢¼: shape={query_embedding.shape}")

            # Search
            print("   æ­£åœ¨æœå°‹FAISSç´¢å¼•...")
            top_k = 5
            scores, indices = index.search(query_embedding, top_k)
            print(f"   âœ… æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(indices[0])} å€‹çµæœ")

            # Collect results
            results = []
            for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):
                if idx == -1:
                    continue

                idx = int(idx)
                if idx not in index_to_id:
                    print(f"   âš ï¸  è­¦å‘Š: ç´¢å¼• {idx} ä¸åœ¨æ˜ å°„ä¸­")
                    continue

                chunk_id = index_to_id[idx]
                chunk_data = metadata.get(chunk_id, {})

                result = {
                    'rank': rank,
                    'score': float(score),
                    'chunk_id': chunk_id,
                    'text': chunk_data.get('text', ''),
                    'summary': chunk_data.get('summary', ''),
                    'primary_category': chunk_data.get('primary_category', ''),
                    'document_id': chunk_data.get('document_id', '')
                }
                results.append(result)

            all_results[qid] = {
                'query': query_text,
                'results': results
            }

            # é¡¯ç¤ºçµæœ
            print(f"\nğŸ“Š æª¢ç´¢çµæœåˆ†æ:")
            if results:
                scores_list = [r['score'] for r in results]
                print(f"   æ‰¾åˆ° {len(results)} å€‹ç›¸é—œchunks")
                print(f"   æœ€é«˜åˆ†æ•¸: {max(scores_list):.4f}")
                print(f"   å¹³å‡åˆ†æ•¸: {np.mean(scores_list):.4f}")
                print(f"   åˆ†æ•¸ç¯„åœ: {min(scores_list):.4f} - {max(scores_list):.4f}")

                print(f"\n   ğŸ† Top 3 çµæœ:")
                for result in results[:3]:
                    print(f"\n   æ’å {result['rank']} | åˆ†æ•¸: {result['score']:.4f}")
                    print(f"   åˆ†é¡: {result['primary_category']}")
                    print(f"   æ‘˜è¦: {result['summary'][:120]}...")
                    print(f"   å…§å®¹: {result['text'][:150]}...")
            else:
                print(f"   âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçµæœ")

        except Exception as e:
            print(f"   âŒ è™•ç†æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

    # ç¸½é«”åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š ç¸½é«”å“è³ªåˆ†æ")
    print("=" * 80)

    total_queries = len(queries)
    successful_queries = sum(1 for r in all_results.values() if r.get('results'))

    print(f"\nâœ… æˆåŠŸç‡: {successful_queries}/{total_queries} ({successful_queries/total_queries*100:.1f}%)")

    if successful_queries > 0:
        all_scores = []
        all_top_scores = []

        for qid, data in all_results.items():
            if data.get('results'):
                scores = [r['score'] for r in data['results']]
                all_scores.extend(scores)
                all_top_scores.append(scores[0])

        print(f"\nåˆ†æ•¸çµ±è¨ˆ:")
        print(f"  æ•´é«”å¹³å‡åˆ†æ•¸: {np.mean(all_scores):.4f}")
        print(f"  Topçµæœå¹³å‡åˆ†æ•¸: {np.mean(all_top_scores):.4f}")
        print(f"  æœ€é«˜åˆ†æ•¸: {max(all_scores):.4f}")
        print(f"  æœ€ä½åˆ†æ•¸: {min(all_scores):.4f}")

        # å“è³ªè©•ç´š
        avg_top_score = np.mean(all_top_scores)
        print(f"\nğŸ¯ å“è³ªè©•ç´š:")
        if avg_top_score > 0.7:
            grade = "å„ªç§€ â­â­â­â­â­"
            assessment = "æª¢ç´¢å“è³ªå¾ˆé«˜ï¼Œæ¨¡å‹èƒ½æº–ç¢ºæ‰¾åˆ°ç›¸é—œå…§å®¹"
        elif avg_top_score > 0.5:
            grade = "è‰¯å¥½ â­â­â­â­"
            assessment = "æª¢ç´¢å“è³ªä¸éŒ¯ï¼Œä½†ä»æœ‰æ”¹é€²ç©ºé–“"
        elif avg_top_score > 0.3:
            grade = "ä¸€èˆ¬ â­â­â­"
            assessment = "æª¢ç´¢å“è³ªä¸€èˆ¬ï¼Œéœ€è¦å„ªåŒ–"
        else:
            grade = "éœ€è¦æ”¹é€² â­â­"
            assessment = "æª¢ç´¢å“è³ªè¼ƒä½ï¼Œå»ºè­°èª¿æ•´ç­–ç•¥"

        print(f"  ç­‰ç´š: {grade}")
        print(f"  è©•ä¼°: {assessment}")

        # åˆ†é¡åˆ†æ
        print(f"\nğŸ“‘ æª¢ç´¢å…§å®¹åˆ†é¡åˆ†å¸ƒ:")
        category_counts = {}
        for data in all_results.values():
            if data.get('results'):
                for result in data['results'][:3]:  # åªçœ‹top 3
                    cat = result['primary_category']
                    category_counts[cat] = category_counts.get(cat, 0) + 1

        for cat, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  {cat}: {count} æ¬¡")

    # ä¿å­˜çµæœ
    output_file = BACKEND_DIR / "minimal_test_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'test_config': {
                'embedding_model': model_name,
                'embedding_dir': str(embedding_dir),
                'top_k': 5
            },
            'results': all_results,
            'summary': {
                'total_queries': total_queries,
                'successful_queries': successful_queries,
                'success_rate': successful_queries / total_queries if total_queries > 0 else 0,
                'avg_top_score': np.mean(all_top_scores) if successful_queries > 0 else 0
            }
        }, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜åˆ°: {output_file}")
    print("\nâœ… æ¸¬è©¦å®Œæˆ!")
    print("=" * 80)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\n\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
