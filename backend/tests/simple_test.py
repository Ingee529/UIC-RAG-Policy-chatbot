#!/usr/bin/env python3
"""
ç°¡å–®çš„å–®åŸ·è¡Œç·’RAGæ¸¬è©¦è…³æœ¬
é¿å…å¤šåŸ·è¡Œç·’é–æ­»å•é¡Œ
"""
import os
import sys
import json
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"

# ç¦ç”¨FAISSçš„å¤šåŸ·è¡Œç·’
import faiss
faiss.omp_set_num_threads(1)

from pathlib import Path
import numpy as np

# Add backend to path
BACKEND_DIR = Path(__file__).parent
sys.path.insert(0, str(BACKEND_DIR))

from sentence_transformers import SentenceTransformer
import pickle

def load_backend(embedding_dir):
    """è¼‰å…¥RAGå¾Œç«¯ï¼Œå–®åŸ·è¡Œç·’ç‰ˆæœ¬"""
    print("ğŸ”§ è¼‰å…¥RAGå¾Œç«¯ (å–®åŸ·è¡Œç·’æ¨¡å¼)...")

    # Load FAISS index
    index_path = embedding_dir / "index.faiss"
    print(f"ğŸ“‚ è¼‰å…¥FAISSç´¢å¼•: {index_path}")
    index = faiss.read_index(str(index_path))
    print(f"âœ… FAISSç´¢å¼•å·²è¼‰å…¥: {index.ntotal} å€‹å‘é‡")

    # Load ID mapping
    id_mapping_path = embedding_dir / "id_mapping.pkl"
    print(f"ğŸ“‚ è¼‰å…¥IDæ˜ å°„: {id_mapping_path}")
    with open(id_mapping_path, 'rb') as f:
        mapping_data = pickle.load(f)
        index_to_id = mapping_data['index_to_id']
    print(f"âœ… IDæ˜ å°„å·²è¼‰å…¥: {len(index_to_id)} å€‹æ˜ å°„")

    # Load metadata
    metadata_path = embedding_dir / "metadata.json"
    print(f"ğŸ“‚ è¼‰å…¥å…ƒæ•¸æ“š: {metadata_path}")
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    print(f"âœ… å…ƒæ•¸æ“šå·²è¼‰å…¥: {len(metadata)} å€‹chunks")

    # Load embedding model
    model_name = metadata.get('model_name', 'Snowflake/arctic-embed-s')
    print(f"ğŸ“‚ è¼‰å…¥embeddingæ¨¡å‹: {model_name}")
    model = SentenceTransformer(model_name)
    print(f"âœ… Embeddingæ¨¡å‹å·²è¼‰å…¥")

    return {
        'index': index,
        'index_to_id': index_to_id,
        'metadata': metadata,
        'model': model
    }

def retrieve(backend, query, top_k=5):
    """æª¢ç´¢ç›¸é—œchunks"""
    print(f"\nğŸ” æŸ¥è©¢: {query}")

    # Encode query
    query_embedding = backend['model'].encode([query], convert_to_numpy=True)

    # Search
    scores, indices = backend['index'].search(query_embedding, top_k)

    # Collect results
    results = []
    for score, idx in zip(scores[0], indices[0]):
        if idx == -1:
            continue

        idx = int(idx)
        chunk_id = backend['index_to_id'][idx]
        chunk_data = backend['metadata'][chunk_id]

        results.append({
            'chunk_id': chunk_id,
            'score': float(score),
            'text': chunk_data.get('text', '')[:200] + '...',  # åªé¡¯ç¤ºå‰200å­—
            'summary': chunk_data.get('summary', ''),
            'primary_category': chunk_data.get('primary_category', ''),
            'document_id': chunk_data.get('document_id', '')
        })

    return results

def analyze_quality(query, results):
    """åˆ†ææª¢ç´¢å“è³ª"""
    print(f"\nğŸ“Š å“è³ªåˆ†æ - {query}")
    print(f"æª¢ç´¢åˆ° {len(results)} å€‹çµæœ")

    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçµæœ")
        return {
            'has_results': False,
            'avg_score': 0,
            'top_score': 0
        }

    scores = [r['score'] for r in results]
    avg_score = np.mean(scores)
    top_score = scores[0] if scores else 0

    print(f"å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•¸: {avg_score:.4f}")
    print(f"æœ€é«˜åˆ†æ•¸: {top_score:.4f}")
    print(f"åˆ†æ•¸ç¯„åœ: {min(scores):.4f} - {max(scores):.4f}")

    # é¡¯ç¤ºtop 3çµæœ
    print("\nğŸ† Top 3 çµæœ:")
    for i, result in enumerate(results[:3], 1):
        print(f"\nçµæœ {i} (åˆ†æ•¸: {result['score']:.4f}):")
        print(f"  åˆ†é¡: {result['primary_category']}")
        print(f"  æ‘˜è¦: {result['summary'][:100]}...")
        print(f"  æ–‡æœ¬é è¦½: {result['text'][:150]}...")

    return {
        'has_results': True,
        'num_results': len(results),
        'avg_score': avg_score,
        'top_score': top_score,
        'score_range': (min(scores), max(scores)),
        'categories': [r['primary_category'] for r in results[:3]]
    }

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 80)
    print("ğŸš€ UIC Policy Assistant - ç°¡å–®æ¸¬è©¦")
    print("=" * 80)

    # è¼‰å…¥æ¸¬è©¦å•é¡Œ
    queries_file = BACKEND_DIR / "test_queries.json"
    print(f"\nğŸ“‚ è¼‰å…¥æ¸¬è©¦å•é¡Œ: {queries_file}")
    with open(queries_file, 'r') as f:
        queries = json.load(f)
    print(f"âœ… è¼‰å…¥äº† {len(queries)} å€‹æ¸¬è©¦å•é¡Œ")

    # è¼‰å…¥å¾Œç«¯
    embedding_dir = BACKEND_DIR / "embeddings_output" / "naive" / "naive_embedding"
    backend = load_backend(embedding_dir)

    # åŸ·è¡Œæª¢ç´¢ä¸¦åˆ†æ
    all_quality = {}
    print("\n" + "=" * 80)
    print("ğŸ“ é–‹å§‹æ¸¬è©¦")
    print("=" * 80)

    for qid, query in queries.items():
        print(f"\n{'='*80}")
        results = retrieve(backend, query, top_k=5)
        quality = analyze_quality(query, results)
        all_quality[qid] = quality

    # ç¸½é«”å“è³ªåˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š ç¸½é«”å“è³ªåˆ†æ")
    print("=" * 80)

    has_results_count = sum(1 for q in all_quality.values() if q['has_results'])
    print(f"\næˆåŠŸæª¢ç´¢ç‡: {has_results_count}/{len(queries)} ({has_results_count/len(queries)*100:.1f}%)")

    if has_results_count > 0:
        avg_scores = [q['avg_score'] for q in all_quality.values() if q['has_results']]
        top_scores = [q['top_score'] for q in all_quality.values() if q['has_results']]

        print(f"æ•´é«”å¹³å‡åˆ†æ•¸: {np.mean(avg_scores):.4f}")
        print(f"æœ€é«˜topåˆ†æ•¸: {max(top_scores):.4f}")
        print(f"æœ€ä½topåˆ†æ•¸: {min(top_scores):.4f}")

        # è©•åˆ†æ¨™æº–
        print("\nğŸ¯ å“è³ªè©•ç´š:")
        overall_score = np.mean(avg_scores)
        if overall_score > 0.7:
            print("âœ… å„ªç§€ - æª¢ç´¢å“è³ªå¾ˆé«˜")
        elif overall_score > 0.5:
            print("âš ï¸  è‰¯å¥½ - æª¢ç´¢å“è³ªå°šå¯ï¼Œæœ‰æ”¹é€²ç©ºé–“")
        elif overall_score > 0.3:
            print("âš ï¸  ä¸€èˆ¬ - æª¢ç´¢å“è³ªéœ€è¦æ”¹é€²")
        else:
            print("âŒ è¼ƒå·® - æª¢ç´¢å“è³ªè¼ƒä½ï¼Œéœ€è¦å„ªåŒ–")

    # ä¿å­˜çµæœ
    output_file = BACKEND_DIR / "simple_test_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'queries': queries,
            'quality_metrics': all_quality,
            'overall': {
                'success_rate': has_results_count / len(queries),
                'avg_score': np.mean(avg_scores) if has_results_count > 0 else 0
            }
        }, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_file}")
    print("\nâœ… æ¸¬è©¦å®Œæˆ!")

if __name__ == "__main__":
    main()
