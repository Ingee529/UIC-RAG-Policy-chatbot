import sys
import json
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np

def main():
    # Read input from stdin
    try:
        input_data = json.load(sys.stdin)
        query = input_data['query']
        chunks = input_data['chunks']
        
        # Load model
        model_name = os.getenv("RERANKER_MODEL_NAME", "BAAI/bge-reranker-base")
        # âœ… ä¿®æ­£èªæ³•ï¼šæ‹¬è™Ÿè¦åœ¨æœ€å¾Œé¢
        print(f"ğŸ”„ Loading reranker model: {model_name}", file=sys.stderr)
        
        # Hugging Face token check
        hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")
        
        if hf_token:
            from huggingface_hub import login
            login(token=hf_token)
            # âœ… ä¿®æ­£èªæ³•
            print(f"âœ… Authenticated with Hugging Face token", file=sys.stderr)
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
        model = AutoModelForSequenceClassification.from_pretrained(model_name, token=hf_token)
        model.eval()
        
        # Use MPS if available
        if torch.backends.mps.is_available():
            device = torch.device("mps")
            # âœ… ä¿®æ­£èªæ³•
            print("ğŸ“± Using Apple Silicon GPU (MPS)", file=sys.stderr)
        elif torch.cuda.is_available():
            device = torch.device("cuda")
            # âœ… è£œä¸Š file=sys.stderr (åŸæœ¬æ¼äº†ï¼Œé€™å¾ˆé‡è¦ï¼)
            print("ğŸ® Using NVIDIA GPU (CUDA)", file=sys.stderr)
        else:
            device = torch.device("cpu")
            # âœ… ä¿®æ­£èªæ³•
            print("ğŸ’» Using CPU", file=sys.stderr)
        
        model.to(device)
        # âœ… ä¿®æ­£èªæ³•
        print(f"âœ… Reranker model loaded successfully on {device}", file=sys.stderr)
        
        # Prepare pairs
        pairs = []
        for chunk in chunks:
            pairs.append([query, chunk['text']])
            
        # Inference
        inputs = tokenizer(
            pairs,
            padding=True,
            truncation=True,
            return_tensors='pt',
            max_length=512
        )
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            logits = model(**inputs).logits
            
        if logits.size(-1) == 2:
            scores = logits[:, 1]
        else:
            scores = logits.view(-1)
            
        scores = scores.cpu().numpy().tolist()
        
        # Output result (é€™æ˜¯å”¯ä¸€éœ€è¦å°åˆ° stdout çš„ JSONï¼Œä¿æŒåŸæ¨£)
        print(json.dumps({"scores": scores}))
        
    except Exception as e:
        # Error ä¹Ÿå°æˆ JSON åˆ° stdoutï¼Œè®“ä¸»ç¨‹å¼æ¥ä½
        print(json.dumps({"error": str(e)}))
        sys.exit(1)

if __name__ == "__main__":
    main()